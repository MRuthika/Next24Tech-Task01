# -*- coding: utf-8 -*-
"""Task01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/141nlQOeariCVeae86gL2vFakHL7yDTjp
"""

pip install pandas numpy seaborn matplotlib scikit-learn

"""**Data Collection**"""

import pandas as pd

# Load the Ames Housing dataset
try:
    dataset = '/content/AmesHousing.csv'
    data = pd.read_csv(dataset)
    print("Data loaded successfully.")
except Exception as e:
    print(f"Error loading data: {e}")

# Display the first few rows of the dataset
print("Initial Data Preview:")
print(data.head())

"""**Data Preprocessing**"""

# Data Preprocessing
try:
    # Handle missing values for numerical columns
    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns
    data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())

    # Handle missing values for categorical columns
    categorical_cols = data.select_dtypes(include=['object']).columns
    data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])

    # Encode categorical variables using one-hot encoding
    data = pd.get_dummies(data, drop_first=True)
    print("Data preprocessing completed successfully.")
except Exception as e:
    print(f"Error during preprocessing: {e}")

"""**Feature Engineering**"""

# Feature Engineering
try:
    # Apply log transformation to the target variable (SalePrice)
    data['Log_SalePrice'] = np.log1p(data['SalePrice'])

    # Create a new feature: House Age
    data['HouseAge'] = data['YrSold'] - data['YearBuilt']

    # Drop less relevant or redundant features
    data = data.drop(['Id', 'SalePrice', 'YearBuilt', 'YrSold'], axis=1)
    print("Feature engineering completed successfully.")
except Exception as e:
    print(f"Error during feature engineering: {e}")

"""**Model Selection**"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Model Selection
try:
    # Define features (X) and target variable (y)
    X = data.drop('Log_SalePrice', axis=1)
    y = data['Log_SalePrice']

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize models
    models = {
        "Linear Regression": LinearRegression(),
        "Random Forest": RandomForestRegressor(),
        "Gradient Boosting": GradientBoostingRegressor()
    }

    # Train and evaluate models
    print("\nModel Performance:")
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        print(f'{name} - RMSE: {rmse}, MAE: {mae}, R-squared: {r2}')
except Exception as e:
    print(f"Error during model selection and evaluation: {e}")

"""**Model Evaluation**"""

# Detailed Evaluation of Gradient Boosting Model
try:
    gb_model = GradientBoostingRegressor()
    gb_model.fit(X_train, y_train)
    y_pred = gb_model.predict(X_test)

    # Calculate evaluation metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print("\nGradient Boosting Evaluation Metrics:")
    print(f'RMSE: {rmse}')
    print(f'MAE: {mae}')
    print(f'R-squared: {r2}')
except Exception as e:
    print(f"Error during detailed model evaluation: {e}")

"""**Visualization and Interpretation**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Visualization and Interpretation
try:
    # Feature Importance for Gradient Boosting
    importances = gb_model.feature_importances_
    indices = np.argsort(importances)[::-1]

    plt.figure(figsize=(12, 8))
    sns.barplot(y=X.columns[indices], x=importances[indices])
    plt.title('Feature Importance')
    plt.show()

    # Actual vs Predicted Prices
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=y_pred)
    plt.xlabel('Actual Log Prices')
    plt.ylabel('Predicted Log Prices')
    plt.title('Actual vs Predicted House Prices')
    plt.show()

    # Residual Analysis
    residuals = y_test - y_pred
    plt.figure(figsize=(10, 6))
    sns.histplot(residuals, kde=True)
    plt.title('Residuals Distribution')
    plt.show()
except Exception as e:
    print(f"Error during visualization and interpretation: {e}")